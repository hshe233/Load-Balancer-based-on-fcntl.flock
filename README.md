# Load-Balancer-based-on-fcntl.flock
1 背景介绍
1.1 IMPALA-4692
Currently the HS2 protocol that most of clients use does not contain the session id in the messages, thus the load balancers need to fall back on the source ip if they want to make sure that the same session is routed to the same coordinator node.
Please include the session id in each message, in order to open up the possibility to implement a “level 7” load balancer that routes the traffic based on that.

目前大多数IMPALA客户端使用的HS2协议在通信过程中都不会包含会话ID信息，因此负载均衡器需要回溯客户端的IP地址，来使得同一客户端的请求被提交到同一个协作节点。
如果要实现七层负载均衡，需要在通信过程中加入会话ID信息。

说明：
基于客户端IP地址和端口号实现的是四层负载均衡，在此基础上，再考虑应用层的特性，来实现七层的负载均衡，常用的方式是不同的应用使用不同的会话ID来进行区分。
1.2 大数据平台日常跑批流程
简要过程如下：
	•	由两台加载节点负责调用跑批脚本，向F5负载均衡服务器提交执行请求。
	•	F5负载均衡服务器将请求分发到大数据平台系统各数据节点（30个）执行。
	•	各节点之间定时进行元数据同步，使元数据信息保持一致。


1.3 流程缺陷及影响
由于IMPALA-4692，当下的传输协议中不包含会话ID，无法实现基于应用的七层负载均衡。又因为跑批加载节点只有2台，如果基于IP和端口号做四层负载均衡，那么相应的也只能使用到2台服务器进行跑批，这显然是无法满足性能需求的。

由此，对大数据平台日常跑批产生了如下影响：
	•	无保持会话
基于F5的负载均衡方案下，客户端和服务端的会话是无状态的，彼此之间不做会话保持。跑批客户端有一超时重加载机制，当某次服务器超过时间阈值（默认为45s）未响应时，会重发SQL请求，但由于网络延迟等原因，服务端可能已经完成了上一个请求并关闭了会话。此时重发的请求便会以“Session Closed”为由被拒绝，造成脚本报错。
	•	同一脚本跨节点元数据同步延迟
F5分发的任务单元是SQL语句级别的，同一个脚本中的多段SQL语句被随机提交到各个数据节点上执行。当有依赖关系的两段SQL被提交到不同数据节点，而节点间元数据同步在高并发的情况下呈现出一定的延迟，SQL之间依赖的数据无法获取，导致执行错误。

缺陷1会引发跑批报错，且偶发性较强。
缺陷2有时不引发报错，但会引起数据不准。
这会引发跑批进度和数据质量两方面的问题，不仅加大了运维压力，也对下游供数、业务取数等多个应用场景产生影响。
2 解决方案
2.1 问题分析
针对IMPALA-4692 Issue，结合大数据平台本身的跑批流程进行分析，我们认为该问题的解决有两种思路：
	•	在HS2协议的传输过程中加入基于脚本的会话ID，并对F5进行设置，实现七层负载均衡。
	•	设计一种F5替代方案，使得加载节点执行脚本时直接连接到IMPALA某个节点，打开一个应用级别的会话，并顺序执行完所有SQL语句块，然后关闭会话。

经过对比，方案1需要同时对Impyla客户端、HS2协议、服务端、F5节点进行改造，涉及范围较大，执行难度较高。方案2只需要改造负载均衡，而负载均衡方案业界有较多实例可供参考，相对而言较易实现。
2.2 基于Python的软负载均衡
负载均衡的本质就是要使执行任务被合理切分到集群中的各台服务器上。基于此，我们提出了一套使用Python实现的软负载均衡方案：
	•	在加载节点的Impyla客户端中新增Balancer类，在Balancer类中实现了getHost()方法，读取文件锁中记录的当前节点编号，之后写入下一节点编号。
	•	新增Lock类，实现文件锁功能，对文件node_in_use.tmp加锁，同一时间只允许一个线程打开并编辑该文件。
	•	修正原本的Impyla客户端代码，在获取服务器连接时，不再连接到F5负载均衡服务器，而是调用Balancer.getHost()方法，自动返回锁文件中记录的服务端IP地址。
	•	将锁文件记录的IP地址指向地址列表中的下一个IMPALA节点。

其中，IP地址列表记录在配置文件中。此外，因为有两个加载节点，因此将锁文件和配置文件都放在NAS盘上共用。
配置文件中同时记录锁文件名称、服务端节点数、节点IP。

3 总结
3.1 流程优化
引入以上方法后，大数据平台跑批流程变更为以下形式：
	•	加载节点从锁文件中获取一个服务器IP地址
	•	连接到指定服务器并执行脚本所有语句
	•	关闭会话
	•	各节点之间仍定时进行元数据同步

新的流程具有以下优势：
	•	脚本运行时候直接连接到服务端打开会话，客户端的会话状态和服务端的会话状态保持一致。规避了因会话异常关闭引起的报错。
	•	同一脚本的所有语句执行在同一数据节点，规避了因元数据同步延迟带来的依赖错误。
3.2 效果评价
	•	方案部署后，2个加载节点的脚本能够按顺序循环执行在大数据平台30个数据节点上，实现了轮询方式的软负载均衡。
	•	从锁文件获取IP地址的过程十分快速（毫秒级），基本不存在排队等待时间。
	•	上线后运行至今近一个月，未再出现前文提及的两种错误，可以视为由IMPALA-4692 Issue引发的缺陷已经得到修复。
	•	提升了大数据平台的跑批效率和数据质量，减缓了运维压力。
3.3 拓展应用
传统的F5负载均衡方案有一定的硬件成本，且随着服务容量的上升，F5节点的压力也越来越大。使用Nginx、LVS、HAProxy等基于Linux的软件需要安装和配置服务，不利于一般开发人员快速使用。
该方案是轻量级的软负载均衡实现，使用Python开发，在Python工程中可以直接调用，在java工程中可以通过引入jpython.jar来进行调用。对于有负载均衡需求的开发测试或者规模较小的网络服务来说，可以实现快速引入，减少开发工作量和硬件成本。
